{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a03f0c-0b50-4317-9e08-67f48e444a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11811833-4885-42d7-88b4-de5dc0f35d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa2d = load_dataset(\"domenicrosati/QA2D\", split=\"train[:50000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d8dac0-aa36-4bd6-8952-965e90d28af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa2d = qa2d.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9bb0326-8aec-48a6-9953-33643ec4eab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'SQuAD',\n",
       " 'example_uid': '572fff11947a6a140053cf1e',\n",
       " 'question': 'Which interstate is the only one that uses kilometer posts instead of mileposts ?',\n",
       " 'answer': 'I-19',\n",
       " 'turker_answer': 'I-19 is the only interstate that uses kilometer posts instead of mileposts .',\n",
       " 'rule-based': 'The only one that uses kilometer posts instead of mileposts is i-19 .'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa2d[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0929e038-8b68-493c-8ee9-9312b12a8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(example):\n",
    "    \"\"\"\n",
    "    Make sure that the samples used for training\n",
    "    1. Have an answer\n",
    "    2. Have a question \n",
    "    3. Has an answer (turker_answer)\n",
    "    \"\"\"\n",
    "    usable = example[\"answer\"] != \"\" and example[\"question\"] != \"\" and example[\"turker_answer\"] != \"\"\n",
    "    return usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d83249d0-812a-4285-aced-863aa5529e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9929891919644126927fa46788fb6d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c66e9264f34c7a80e25416e08cb9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dataset', 'example_uid', 'question', 'answer', 'turker_answer', 'rule-based'],\n",
       "        num_rows: 45000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['dataset', 'example_uid', 'question', 'answer', 'turker_answer', 'rule-based'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa2d.filter(filter_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af0855a5-e404-4b5d-adf6-56ab79d99b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google/flan-t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a3d4ca5-9276-49c6-9dcc-54e0ad6b1111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3819e25c6916423aa5a5559b86babd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MyProjects\\DawsonAI\\.env\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\hf_cache\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aecec77fc6e4757a2ae8316646e2b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f808128b9d4741a825724c74e206a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c03f4c0de94177bd9f5278ab5dc1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "811d41dd-6326-431e-8e5a-45f950806e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_targets(examples):\n",
    "    # The output is of format \"question? answer\"\n",
    "    return list(map(\n",
    "        lambda q, a: f\"{q} {a}\",\n",
    "        examples[\"question\"],\n",
    "        examples[\"answer\"],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8899f2d5-720f-4aa7-b5e1-4faaa663f9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Which interstate is the only one that uses kilometer posts instead of mileposts ? I-19',\n",
       " 'What kind of topics began appearing more commonly in poetry and literature during the Enlightenment ? scientific topics',\n",
       " 'What is a major reason that Bermuda has problems with overpopulation ? limited land area ,']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_targets(qa2d[\"train\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "818b5ae9-4706-43b0-8aa1-438645998265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\" \n",
    "    The objective of our model is to transform a sentence into a question.\n",
    "    To do so, the input to the model will be the sentence itself.\n",
    "    The output of the model is be the question followed by the answer.\n",
    "    \"\"\"\n",
    "    prompt = \"ask: \" # Each input will be formatted as \"ask: sentence...\"\n",
    "    inputs = [prompt + x for x in examples[\"turker_answer\"]] \n",
    "    # The output is of format \"question? answer\"\n",
    "    targets = generate_targets(examples)\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74ae580a-73f2-4540-b4e3-fa48e60a84e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author mainly wants to show the competitive advantages of Melbourne in the passage . => What does the author mainly want to show in the passage ? The competitive advantages of Melbourne .\n",
      "Sony includes call Of Duty 3 in their low-end price range . => Which Call of Duty title does Sony include in their low-end price range ? Call Of Duty 3\n",
      "People communicate with dogs by voice commands , body language or posture and hand signals . => People communicate with dogs by voice commands , body language or posture and what else ? hand signals\n",
      "Lew Wasserman was the head of MCA in 1990 . => Who was the head of MCA in 1990 ? Lew Wasserman\n"
     ]
    }
   ],
   "source": [
    "statements = qa2d[\"train\"][20:24]\n",
    "for sample in zip(statements[\"turker_answer\"], generate_targets(statements)):\n",
    "    print(f\"{sample[0]} => {sample[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ace8731-1012-4d4e-a318-3809baa7ba2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b95c84436cd40fe8a2ecb80cfdc6ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6004688f8d4076925ce3970695c7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_qa2d = qa2d.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14858f29-ac7d-4639-8907-55fb651561f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f486ea72-f928-4151-abbe-a8a9b9ffe42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fb337a5-c3f3-4507-9de6-2e719a66eabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6c594de1604b4e9ab89a2b884885d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52e28d04-e9ab-42e4-a56c-695a2419ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results_qa2d_t5_flan\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_qa2d[\"train\"],\n",
    "    eval_dataset=tokenized_qa2d[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b707752c-ead7-4e26-a90e-0bf1e8a47a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='11252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   35/11252 04:08 < 23:29:50, 0.13 it/s, Epoch 0.01/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\MyProjects\\DawsonAI\\.env\\Lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\MyProjects\\DawsonAI\\.env\\Lib\\site-packages\\transformers\\trainer.py:2014\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2012\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_global_grad_norm()\n\u001b[0;32m   2013\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2014\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m \u001b[43m_grad_norm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m _grad_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2016\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[0;32m   2017\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "13bebad6-bcdb-49b3-ae3f-118e80e7531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "mcq_model = AutoModelForSeq2SeqLM.from_pretrained(\"results_qa2d/checkpoint-11252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aad7488e-4cc1-456d-a810-911b680079dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the chair conformation of cyclochexane more stable than its boat conformation?\n"
     ]
    }
   ],
   "source": [
    "# Don't forget to add the \"ask: \" prompt!\n",
    "input_text = \"ask: The chair conformation of cyclochexane is more stable than its boat conformation.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = mcq_model.generate(input_ids, max_new_tokens=100, do_sample=False)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5357aa80-bb56-4394-b96f-da90db52ef67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23125ca2-e6bd-4625-90c9-ba2c939d533c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
