{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8a03f0c-0b50-4317-9e08-67f48e444a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11811833-4885-42d7-88b4-de5dc0f35d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa2d = load_dataset(\"domenicrosati/QA2D\", split=\"train[:50000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9d8dac0-aa36-4bd6-8952-965e90d28af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa2d = qa2d.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9bb0326-8aec-48a6-9953-33643ec4eab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'SQuAD',\n",
       " 'example_uid': '57314737497a881900248d3c',\n",
       " 'question': 'What is the name of a type of dual purpose fighter-bomber aircraft used by the US Air Force ?',\n",
       " 'answer': 'F-16 Fighting Falcon',\n",
       " 'turker_answer': 'The F-16 Fighting Falcon is the name of a type of dual purpose fighter-bomber aircraft used by the US Air Force .',\n",
       " 'rule-based': 'The name of a type of dual purpose fighter-bomber aircraft used by the US Air Force is F-16 Fighting Falcon .'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa2d[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0929e038-8b68-493c-8ee9-9312b12a8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(example):\n",
    "    \"\"\"\n",
    "    Make sure that the samples used for training\n",
    "    1. Have an answer\n",
    "    2. Have a question \n",
    "    3. Has an answer (turker_answer)\n",
    "    \"\"\"\n",
    "    usable = example[\"answer\"] != \"\" and example[\"question\"] != \"\" and example[\"turker_answer\"] != \"\"\n",
    "    return usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d83249d0-812a-4285-aced-863aa5529e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044ff75aee1043eead22bd44b379b6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9024ea5049d6465ab4e610e03d49c441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dataset', 'example_uid', 'question', 'answer', 'turker_answer', 'rule-based'],\n",
       "        num_rows: 45000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['dataset', 'example_uid', 'question', 'answer', 'turker_answer', 'rule-based'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa2d.filter(filter_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af0855a5-e404-4b5d-adf6-56ab79d99b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a3d4ca5-9276-49c6-9dcc-54e0ad6b1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "811d41dd-6326-431e-8e5a-45f950806e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_targets(examples):\n",
    "    # The output is of format \"question? answer\"\n",
    "    return list(map(\n",
    "        lambda q, a: f\"{q} {a}\",\n",
    "        examples[\"question\"],\n",
    "        examples[\"answer\"],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8899f2d5-720f-4aa7-b5e1-4faaa663f9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the name of a type of dual purpose fighter-bomber aircraft used by the US Air Force ? F-16 Fighting Falcon',\n",
       " 'What month did Bell go to Boston ? April',\n",
       " 'What was the driving force behind the revitalization of the Roman naval forces ? to meet several new demands']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_targets(qa2d[\"train\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "818b5ae9-4706-43b0-8aa1-438645998265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\" \n",
    "    The objective of our model is to transform a sentence into a question.\n",
    "    To do so, the input to the model will be the sentence itself.\n",
    "    The output of the model is be the question followed by the answer.\n",
    "    \"\"\"\n",
    "    prompt = \"ask: \" # Each input will be formatted as \"ask: sentence...\"\n",
    "    inputs = [prompt + x for x in examples[\"turker_answer\"]] \n",
    "    # The output is of format \"question? answer\"\n",
    "    targets = generate_targets(examples)\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74ae580a-73f2-4540-b4e3-fa48e60a84e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack Brickhouse manned the Cubs radio and TV booth for parts of five decades . => Who manned the Cubs radio and TV booth for parts of five decades ? Jack Brickhouse\n",
      "The refractive index of air is n = 1 . => What is the refractive index of air ? n = 1\n",
      "27 % of European digital satellite TV homes were watching HD broadcast in 2010 . => What percentage of European digital satellite TV homes were watching HD broadcasts in 2010 ? 27 %\n",
      "They have an opportunity to appeal charges . => What did they have an opportunity to appeal ? charges\n"
     ]
    }
   ],
   "source": [
    "statements = qa2d[\"train\"][20:24]\n",
    "for sample in zip(statements[\"turker_answer\"], generate_targets(statements)):\n",
    "    print(f\"{sample[0]} => {sample[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ace8731-1012-4d4e-a318-3809baa7ba2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d714234f21e842fe990d3f1b0375214b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044ef001cd924e17b63bb9db5594a43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_qa2d = qa2d.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "14858f29-ac7d-4639-8907-55fb651561f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f486ea72-f928-4151-abbe-a8a9b9ffe42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MyProjects\\DawsonAI\\.env\\Lib\\site-packages\\transformers\\utils\\hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Popular text2text generation tasks are machine translation, \n",
    "commonly evaluated with the BLEU score and a focus on word precision, \n",
    "and text summarization, commonly evaluated with the ROUGE score and a focus on word recall.\n",
    "\"\"\"\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1fb337a5-c3f3-4507-9de6-2e719a66eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52e28d04-e9ab-42e4-a56c-695a2419ddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MyProjects\\DawsonAI\\.env\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results_qa2d\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_qa2d[\"train\"],\n",
    "    eval_dataset=tokenized_qa2d[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b707752c-ead7-4e26-a90e-0bf1e8a47a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11252' max='11252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11252/11252 48:45, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.578900</td>\n",
       "      <td>0.557558</td>\n",
       "      <td>0.829700</td>\n",
       "      <td>0.665500</td>\n",
       "      <td>0.744600</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>17.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.585900</td>\n",
       "      <td>0.524811</td>\n",
       "      <td>0.840300</td>\n",
       "      <td>0.688100</td>\n",
       "      <td>0.761200</td>\n",
       "      <td>0.761000</td>\n",
       "      <td>17.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.634500</td>\n",
       "      <td>0.512762</td>\n",
       "      <td>0.842800</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.767200</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>17.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.578500</td>\n",
       "      <td>0.509624</td>\n",
       "      <td>0.843500</td>\n",
       "      <td>0.696400</td>\n",
       "      <td>0.767900</td>\n",
       "      <td>0.767700</td>\n",
       "      <td>17.118800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MyProjects\\DawsonAI\\.env\\Lib\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "D:\\MyProjects\\DawsonAI\\.env\\Lib\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "D:\\MyProjects\\DawsonAI\\.env\\Lib\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "D:\\MyProjects\\DawsonAI\\.env\\Lib\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11252, training_loss=0.6435050720046349, metrics={'train_runtime': 2926.2111, 'train_samples_per_second': 61.513, 'train_steps_per_second': 3.845, 'total_flos': 1746638815887360.0, 'train_loss': 0.6435050720046349, 'epoch': 4.0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13bebad6-bcdb-49b3-ae3f-118e80e7531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "q_model = AutoModelForSeq2SeqLM.from_pretrained(\"results_qa2d_t5_small/checkpoint-11252\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"results_qa2d_t5_small/checkpoint-11252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b10341-5ab2-4df1-8e80-c96c6083da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad7488e-4cc1-456d-a810-911b680079dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Dante's birthday? Today\n"
     ]
    }
   ],
   "source": [
    "# Don't forget to add the \"ask: \" prompt!\n",
    "input_text = \"Today it is Dante's birthday.\"\n",
    "# Use NLTK to tokenize the whole text into sentences\n",
    "sentences = sent_tokenize(input_text)\n",
    "for sentence in sentences:\n",
    "    # Don't forget to add our prefix for prompting!\n",
    "    text_input = \"ask: \" + sentence\n",
    "    input_ids = tokenizer(text_input, return_tensors=\"pt\").input_ids\n",
    "    outputs = q_model.generate(input_ids, max_new_tokens=100, do_sample=False)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
